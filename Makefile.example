.PHONY: version_clean version connections run_annotation filter_connections filter_connections_ann merge

#################################################################################
# PROJECT CONFIGURATION (You can change these)                                  #
#################################################################################

PROJECT_NAME = idisk
# Your preferred Python.
PYTHON_INTERPRETER = /path/to/virtual_environment/python
PYTHON_VERSION := $(shell $(PYTHON_INTERPRETER) -V)
# Where iDISK is located.
PROJECT_HOME = /full/path/to/iDISK_home/
# Configuration file specifying how to build the schema.
SCHEMA_VERSION = 1.0.0
# Paths to the source ingredient concept files.
SOURCE_FILES := $(PROJECT_HOME)/sources/DSLD/10_22_2018/import/concepts.jsonl \
	        $(PROJECT_HOME)/sources/NHP/12_1_2017/import/concepts.jsonl 

# These need to be available to a variety of scripts, so we declare them as 
# environment variables.
export PROJECT_VERSION = test
# This directory should contain the following files
#   kb.ini
#   linkers.conf
#   schemas.ini
#   schemas/
export CONFIG_DIR := $(PROJECT_HOME)/lib/config

#################################################################################
# GLOBALS (DON'T CHANGE THESE)                                                  #
#################################################################################

DATENOW := $(shell date +%F)
VERSION_DIR := $(PROJECT_HOME)/versions/$(PROJECT_VERSION)/


#################################################################################
# PROJECT RULES                                                                 #
#################################################################################

## Remove this version
version_clean:
	rm -r $(VERSION_DIR)


## Remove all annotation files and Prodigy datasets.
annotation_clean:
	rm -r $(VERSION_DIR)/concepts/manual_review/indata
	prodigy drop concept_matching


## Create the directories for this version.
version:
	mkdir -p $(VERSION_DIR)/concepts/
	mkdir -p $(VERSION_DIR)/build/{Neo4j,RRF}/
	@echo "cat $(SOURCE_FILES) > $(VERSION_DIR)/concepts/concepts_orig.jsonl" \
		> $(VERSION_DIR)/concepts/LOG
	cat $(SOURCE_FILES) > $(VERSION_DIR)/concepts/concepts_orig.jsonl
	@echo "Project Name: $(PROJECT_NAME)" > $(VERSION_DIR)/VERSION_INFO.txt
	@echo "Project Version: $(PROJECT_VERSION)" >> $(VERSION_DIR)/VERSION_INFO.txt
	@echo "Schema Version: $(SCHEMA_VERSION)" >> $(VERSION_DIR)/VERSION_INFO.txt
	@echo "Python Version: $(PYTHON_VERSION)" >> $(VERSION_DIR)/VERSION_INFO.txt
	@echo "Python Path: $(PYTHON_INTERPRETER)" >> $(VERSION_DIR)/VERSION_INFO.txt
	@echo "Source files:" >> $(VERSION_DIR)/VERSION_INFO.txt
	@for f in $(SOURCE_FILES) ; do \
		echo "  $$f" >> $(VERSION_DIR)/VERSION_INFO.txt ; \
	done
	@echo "# Changes in version $(PROJECT_VERSION)" > $(VERSION_DIR)/CHANGELOG.md
	@echo
	@echo "Version info saved to $(VERSION_DIR)/VERSION_INFO.txt"
	@echo "Fill in the changes for this version in $(VERSION_DIR)/CHANGELOG.md"
	@echo $(PROJECT_VERSION) > $(VERSION_DIR)/.version
	@cp -r $(PROJECT_HOME)/lib/config $(VERSION_DIR)/


## Create the iDISK schema as a neo4j graph
schema:
	$(PYTHON_INTERPRETER) $(PROJECT_HOME)/lib/idlib/idlib/schema.py \
		--schema_version $(SCHEMA_VERSION) \
		--schema_conf_file $(CONFIG_DIR)/schemas.ini

## Check the input data to ensure proper formatting
check_contents:
	$(PYTHON_INTERPRETER) $(PROJECT_HOME)/lib/check_content.py \
		--concept_files $(SOURCE_FILES)

## Link concepts to external terminologies
link_entities:
	$(PYTHON_INTERPRETER) $(PROJECT_HOME)/lib/idlib/idlib/entity_linking/run_entity_linking.py \
		--concepts_file $(VERSION_DIR)/concepts/concepts_orig.jsonl \
		--outfile $(VERSION_DIR)/concepts/concepts_linked.jsonl \
		--linkers_conf $(CONFIG_DIR)/linkers.conf \
		--uri localhost --user neo4j --password password \
		2> $(VERSION_DIR)/concepts/concepts_linked.jsonl.log &
	@echo "Running entity linking. Check $(VERSION_DIR)/concepts/concepts_linked.jsonl.log for progress."
	@echo "Linked concepts will be written to $(VERSION_DIR)/concepts/concepts_linked.jsonl" 


## Find candidate connections between concepts.
connections:
	$(PYTHON_INTERPRETER) $(PROJECT_HOME)/lib/idlib/idlib/set_functions.py find_connections \
		--infiles $(VERSION_DIR)/concepts/concepts_linked.jsonl \
		--outfile $(VERSION_DIR)/concepts/connections.csv \
		--ignore_concept_types DSP \
		2> $(VERSION_DIR)/concepts/connections.csv.log &
	@echo "Finding connections. Check $(VERSION_DIR)/concepts/connections.csv.log for progress."
	@echo "Connections will be written to $(VERSION_DIR)/concepts/connections.csv"


## Filter connections based on simple heuristics
filter_connections:
	$(PYTHON_INTERPRETER) $(PROJECT_HOME)/lib/filter_connections_basic.py \
		--connections_file $(VERSION_DIR)/concepts/connections.csv \
		--concepts_file $(VERSION_DIR)/concepts/concepts_linked.jsonl \
		--outfile $(VERSION_DIR)/concepts/connections_new.csv \
		--ignore_concept_types DSP
	@mv $(VERSION_DIR)/concepts/connections.csv $(VERSION_DIR)/concepts/connections_orig.csv
	@mv $(VERSION_DIR)/concepts/connections.csv.log $(VERSION_DIR)/concepts/connections_orig.csv.log
	@mv $(VERSION_DIR)/concepts/connections_new.csv $(VERSION_DIR)/concepts/connections.csv
	@echo "New connections written to \n\t $(VERSION_DIR)/concepts/connections.csv"
	@echo "Original connections at \n\t $(VERSION_DIR)/concepts/connections_orig.csv"


## Run annotation using Prodigy
run_annotation:
	# Create the annotation data in Prodigy JSON lines format.
	mkdir -p $(VERSION_DIR)/concepts/manual_review/indata
	[[ -f $(VERSION_DIR)/concepts/manual_review/indata/matches.jsonl ]] || \
	$(PYTHON_INTERPRETER) $(PROJECT_HOME)/lib/annotation/to_prodigy.py \
		--concepts_file $(VERSION_DIR)/concepts/concepts_linked.jsonl \
		--connections_file $(VERSION_DIR)/concepts/connections.csv \
		--outfile $(VERSION_DIR)/concepts/manual_review/indata/matches.jsonl
	# Create a dataset to hold the annotations if it doesn't exist.
	cd $(VERSION_DIR)/concepts/manual_review/
	[[ -f prodigy.json ]] || ln -s $(PROJECT_HOME)/lib/annotation/prodigy_resources/prodigy.json . 
	prodigy stats -l | grep -q "concept_matching" && \
		echo "Prodigy dataset already exists. Make sure this is what you want." || \
		prodigy dataset concept_matching "Evaluate whether connected concepts are indeed synonymous."
	# Run annotation.
	prodigy compare concept_matching \
		$(VERSION_DIR)/concepts/manual_review/indata/matches.jsonl \
		$(PROJECT_HOME)/lib/annotation/prodigy_resources/template.html \
		-F $(PROJECT_HOME)/lib/annotation/prodigy_resources/recipe.py


## Filter connections according to annotations.
filter_connections_ann:
	# Save annotations to a file
	prodigy db-out concept_matching $(VERSION_DIR)/concepts/manual_review/annotations
	$(PYTHON_INTERPRETER) $(PROJECT_HOME)/lib/annotation/filter_connections_ann.py \
		--connections_file $(VERSION_DIR)/concepts/connections.csv \
		--annotations_file $(VERSION_DIR)/concepts/manual_review/annotations/concept_matching.jsonl \
		--outfile $(VERSION_DIR)/concepts/connections_new.csv
	@mv $(VERSION_DIR)/concepts/connections.csv $(VERSION_DIR)/concepts/connections_orig.csv
	@mv $(VERSION_DIR)/concepts/connections_new.csv $(VERSION_DIR)/concepts/connections.csv
	@echo "New connections written to \n\t $(VERSION_DIR)/concepts/connections.csv $(VERSION_DIR)/concepts/connections.csv"
	@echo "Original connections at \n\t $(VERSION_DIR)/concepts/connections.csv $(VERSION_DIR)/concepts/connections_orig.csv"


## Merge matched concepts.
merge:
	$(PYTHON_INTERPRETER) $(PROJECT_HOME)/lib/idlib/idlib/set_functions.py union \
		--infiles $(VERSION_DIR)/concepts/concepts_linked.jsonl \
		--connections $(VERSION_DIR)/concepts/connections.csv \
		--outfile $(VERSION_DIR)/concepts/concepts_merged.jsonl
	@echo "Merged concepts written to $(VERSION_DIR)/concepts/concepts_merged.jsonl"


## Populate the Neo4j graph
neo4j:
	$(PYTHON_INTERPRETER) $(PROJECT_HOME)/lib/idlib/idlib/formatters/neo4j.py \
		--concepts_file $(VERSION_DIR)/concepts/concepts_merged.jsonl \
		--uri localhost --user neo4j --password password \
		2> $(VERSION_DIR)/build/Neo4j/build.log &
	@echo "Populating the Neo4j graph at localhost:7687."
	@echo "Check progress in the log file at $(VERSION_DIR)/build/Neo4j/build.log"
	@echo "Export the graph to a binary file using the following command from within the Neo4j terminal:"
	@echo "  bin/neo4j-admin dump --database=graph.db --to=<destination_path>"


## Create the UMLS-style RRF files
rrf:
	$(PYTHON_INTERPRETER) $(PROJECT_HOME)/lib/idlib/idlib/formatters/rrf.py \
		--concepts_file $(VERSION_DIR)/concepts/concepts_merged.jsonl \
		--outdir $(VERSION_DIR)/build/RRF/ \
		2> $(VERSION_DIR)/build/RRF/build.log
	@echo "RRF files written to $(VERSION_DIR)/build/RRF/"


## Package a full release. Includes the Neo4j, RRF, and raw JSON data files and can be loaded directly by idlib.
release:
	cd $(PROJECT_HOME)/versions; \
		zip -r ../releases/$(PROJECT_VERSION)-$(DATENOW).zip $(shell basename $(VERSION_DIR))/
	@echo "Packaged release releases/$(PROJECT_VERSION)-$(DATENOW).zip"


#################################################################################
# Self Documenting Commands                                                     #
#################################################################################

.DEFAULT_GOAL := help

# Inspired by <http://marmelab.com/blog/2016/02/29/auto-documented-makefile.html>
# sed script explained:
# /^##/:
# 	* save line in hold space
# 	* purge line
# 	* Loop:
# 		* append newline + line to hold space
# 		* go to next line
# 		* if line starts with doc comment, strip comment character off and loop
# 	* remove target prerequisites
# 	* append hold space (+ newline) to line
# 	* replace newline plus comments by `---`
# 	* print line
# Separate expressions are necessary because labels cannot be delimited by
# semicolon; see <http://stackoverflow.com/a/11799865/1968>
.PHONY: help
help:
	@echo "$$(tput bold)Available rules:$$(tput sgr0)"
	@echo
	@sed -n -e "/^## / { \
		h; \
		s/.*//; \
		:doc" \
		-e "H; \
		n; \
		s/^## //; \
		t doc" \
		-e "s/:.*//; \
		G; \
		s/\\n## /---/; \
		s/\\n/ /g; \
		p; \
	}" ${MAKEFILE_LIST} \
	| LC_ALL='C' sort --ignore-case \
	| awk -F '---' \
		-v ncol=$$(tput cols) \
		-v indent=19 \
		-v col_on="$$(tput setaf 6)" \
		-v col_off="$$(tput sgr0)" \
	'{ \
		printf "%s%*s%s ", col_on, -indent, $$1, col_off; \
		n = split($$2, words, " "); \
		line_length = ncol - indent; \
		for (i = 1; i <= n; i++) { \
			line_length -= length(words[i]) + 1; \
			if (line_length <= 0) { \
				line_length = ncol - indent - length(words[i]) - 1; \
				printf "\n%*s ", -indent, " "; \
			} \
			printf "%s ", words[i]; \
		} \
		printf "\n"; \
	}' \
	| more $(shell test $(shell uname) = Darwin && echo '--no-init --raw-control-chars')
